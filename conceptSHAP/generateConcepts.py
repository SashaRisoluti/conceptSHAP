import torch
import conceptNet

def train(train_embeddings, train_y_true, clusters, h_x, n_concepts):
    '''
    :param train_embeddings: tensor of sentence embeddings => (# of examples, embedding_dim)
    :param train_y_true: the ground truth label for each of the embeddings => (# of examples)
    :param clusters: tensor of embedding clusters generated by k-means => (# of n_clusters, # of sentences per cluster, embedding_dim)
    :param h_x: final layers of the transformer
    :param n_concepts: number of concepts to generate
    :return: trained conceptModel
    '''

    lr = 0.001
    batch_size = 50
    epochs = 10
    save_interval = 10
    model = conceptNet(clusters, h_x, n_concepts)
    save_dir = 'checkpoints/'
    optimizer = torch.optim.Adam(conceptNet.parameters(), lr=lr)
    train_size = train_embeddings.shape[0]

    for i in range(epochs):
        # TODO might want to shuffle train_embeddings for each new epoch
        batch_start = 0
        batch_end = batch_size
        while batch_end < train_size:
            train_embeddings_narrow = train_embeddings.narrow(0, batch_start, batch_end)
            loss = model.loss(train_embeddings_narrow, train_y_true, regularize=False, l=5.)

            # update gradients
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            # model saving
            if (i + 1) % save_interval == 0:
                state_dict = model.state_dict()
                for key in state_dict.keys():
                    state_dict[key] = state_dict[key].to(torch.device('cpu'))
                    torch.save(state_dict, save_dir /
                           'conceptSHAP_iter_{:d}.pth'.format(i + 1))

            # update batch indices
            batch_start += batch_size
            batch_end += batch_size

if __name__ == "__main__":
    import sys
    sys.path.insert(0, '../')
    from model import bert_inference

    result, embedding = bert_inference.get_sentence_activation()

    # TODO: add testing