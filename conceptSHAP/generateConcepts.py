import torch
from conceptSHAP.conceptNet import ConceptNet
import os
import numpy as np
import pandas as pd
from pytorch_transformers import BertForSequenceClassification
import matplotlib.pyplot as plt


def train(train_embeddings, train_y_true, clusters, h_x, n_concepts):
  '''
  :param train_embeddings: tensor of sentence embeddings => (# of examples, embedding_dim)
  :param train_y_true: the ground truth label for each of the embeddings => (# of examples)
  :param clusters: tensor of embedding clusters generated by k-means => (# of n_clusters, # of sentences per cluster, embedding_dim)
  :param h_x: final layers of the transformer
  :param n_concepts: number of concepts to generate
  :return: trained conceptModel
  '''

  lr = 1E-3
  batch_size = 32
  epochs = 20
  save_interval = 10
  model = ConceptNet(clusters, h_x, n_concepts).cuda()
  save_dir = 'checkpoints/'
  optimizer = torch.optim.Adam(model.parameters(), lr=lr)
  train_size = train_embeddings.shape[0]
  losses = []
  for i in range(epochs):
    if i < 5:  # TODO param here
      regularize = False
    else:
      regularize = True
    batch_start = 0
    batch_end = batch_size
    while batch_end < train_size:
      train_embeddings_narrow = train_embeddings.narrow(0, batch_start, batch_end - batch_start)
      train_y_true_narrow = train_y_true.narrow(0, batch_start, batch_end - batch_start)
      loss = model.loss(train_embeddings_narrow, train_y_true_narrow, regularize=regularize, l=5.)

      losses.append(loss.detach().cpu().item())

      # update gradients
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()

      # # model saving
      # if (i + 1) % save_interval == 0:
      #     state_dict = model.state_dict()
      #     for key in state_dict.keys():
      #         state_dict[key] = state_dict[key].to(torch.device('cpu'))
      #         torch.save(state_dict, save_dir /
      #                'conceptSHAP_iter_{:d}.pth'.format(i + 1))

      # update batch indices
      batch_start += batch_size
      batch_end += batch_size

  return model, losses

if __name__ == "__main__":
  if torch.cuda.is_available():
    device = torch.device('cuda')
    devicename = '[' + torch.cuda.get_device_name(0) + ']'
  else:
    device = torch.device('cpu')
    devicename = ""

  PATH = ""
  assert PATH == "", "fill in path to your intuit-project folder"

  ### LOAD FROM FILES
  # load activation files, expect .npy
  file_name = os.path.join(PATH, 'data/small_activations.npy')
  small_activations = np.load(file_name)
  print(small_activations.shape)

  # load clusters files, expect .npy
  file_name = os.path.join(PATH, 'data/small_clusters.npy')
  small_clusters = np.load(file_name)
  print(small_clusters.shape)

  # load ground truth sentiment label
  small_df = pd.read_pickle(os.path.join(PATH, "Data/sentences_small.pkl"))
  small_df["polarity"] = small_df.shape[0] * [0]
  senti_list = list(small_df['label'])
  senti_list = [1 if i == "positive" else 0 for i in senti_list]
  senti_list = np.array(senti_list)

  # load pretrained model
  bert_model = BertForSequenceClassification.from_pretrained(
    os.path.join(PATH, "imdb_weights"))  # load directly from checkpoint of imdb
  bert_model.to(device)  # move to gpu


  ### PREPARING ARGUMENTS
  # get the embedding numpy array, convert to tensor
  train_embeddings = small_activations  # (4012, 768)
  train_embeddings = torch.from_numpy(train_embeddings).to(device)

  # get the cluster numpy array
  clusters = small_clusters
  clusters = torch.from_numpy(clusters).to(device)

  # get ground truth label
  train_y_true = senti_list
  train_y_true = torch.from_numpy(train_y_true).to(device)

  # h_x
  h_x = list(bert_model.modules())[-1]

  # n_concepts
  n_concepts = 5  # param

  ### TRAIN MODEL
  concept_model, loss = train(train_embeddings, train_y_true, clusters, h_x, n_concepts)
  # plt.plot(loss)
