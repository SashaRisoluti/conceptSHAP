{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert_inference.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arnav-gudibande/intuit-project/blob/master/model/bert_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOXSqynyIvjA",
        "colab_type": "text"
      },
      "source": [
        "## Mounting your google drive file, to load the weights for model\n",
        "\n",
        "0. Make sure you are connected to a GPU run time, if not Runtime -> Change runtime type: select Python3, GPU\n",
        "1. Click the folder icon on the left side\n",
        "2. Run the cell below, follow the instruction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iWwUb9oIvA9",
        "colab_type": "code",
        "outputId": "d6ba1323-54d6-4a42-9946-ad4c6955e17a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOv58Eu4I3RE",
        "colab_type": "text"
      },
      "source": [
        "3. click on folder icon again, there should be two folders \"drive\" and \"sample_data\". If not wait a while and click again. Do not reload the page(since it will just give you a new runtime, thus won't help). The \"drive\" folder will be your google drive root dir.\n",
        "4. If you have not done it yet, find the shared \"intuit-project\" folder and \"add to my drive\". This will add this folder to your google drive root dir, and you should be able to locate that folder now in the file system on the left.\n",
        "5. Find the intuit-project folder, right click and \"copy path\". Put it in the cell below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPMnF_b0IvDJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PATH = '/content/drive/My Drive/intuit-project'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9LSJVrHI_S2",
        "colab_type": "text"
      },
      "source": [
        "# The inference code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpPImfVcd51E",
        "colab_type": "code",
        "outputId": "b1f78878-9201-4572-ad42-43e5c5255dfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "!pip install pytorch_transformers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch_transformers in /usr/local/lib/python3.6/dist-packages (1.2.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (0.0.38)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (2019.12.20)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (0.1.85)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (2.21.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (4.28.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (1.18.1)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (1.4.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (1.12.18)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch_transformers) (0.14.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch_transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch_transformers) (7.1.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_transformers) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_transformers) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_transformers) (3.0.4)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_transformers) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_transformers) (0.9.5)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.18 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_transformers) (1.15.18)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.18->boto3->pytorch_transformers) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.18->boto3->pytorch_transformers) (0.15.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9UkT-MOpNOi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import (TensorDataset, DataLoader,\n",
        "                              RandomSampler, SequentialSampler)\n",
        "from pytorch_transformers import BertTokenizer, BertConfig\n",
        "from pytorch_transformers import BertForSequenceClassification\n",
        "from pytorch_transformers import AdamW, WarmupLinearSchedule\n",
        "from distutils.version import LooseVersion as LV\n",
        "from sklearn.model_selection import train_test_split\n",
        "import io\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "matplotlib.use('Agg')\n",
        "sns.set()\n",
        "import torch.nn as nn\n",
        "import IPython\n",
        "import os\n",
        "e = IPython.embed\n",
        "\n",
        "device = torch.device('cuda')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5n85aGde1Kh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_dataframe(_dframe, _tokenizer):\n",
        "  sentences = _dframe.sentence.values\n",
        "  sentences = [\"[CLS] \" + s for s in sentences] #causing error bc of data format\n",
        "  labels = _dframe.polarity.values #WONT WORK, says no such thing as polarity\n",
        "\n",
        "  tokenized = [_tokenizer.tokenize(s) for s in sentences]\n",
        "  tokenized = [t[:(MAX_LEN_TRAIN-1)]+['SEP'] for t in tokenized]\n",
        "\n",
        "\n",
        "  ids = [_tokenizer.convert_tokens_to_ids(t) for t in tokenized]\n",
        "  ids = np.array([np.pad(i, (0, MAX_LEN_TRAIN-len(i)),\n",
        "                             mode='constant') for i in ids])\n",
        "  amasks = []\n",
        "  for seq in ids:\n",
        "    seq_mask = [float(i>0) for i in seq]\n",
        "    amasks.append(seq_mask)\n",
        " \n",
        "  inputs_reformatted = torch.tensor(ids)\n",
        "  labels_reformatted = torch.tensor(labels)\n",
        "  masks_reformatted = torch.tensor(amasks)\n",
        "\n",
        "  data = TensorDataset(inputs_reformatted, masks_reformatted, labels_reformatted)\n",
        "  sampler = SequentialSampler(data)\n",
        "\n",
        "  dataloader = DataLoader(data, sampler=sampler, batch_size=1)\n",
        "  return dataloader\n",
        "\n",
        "def run_model(_model, loader):\n",
        "\n",
        "  for batch in loader:\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "  with torch.no_grad():\n",
        "      outputs = _model(b_input_ids, token_type_ids=None,\n",
        "                      attention_mask=b_input_mask)\n",
        "      logits = outputs[0]\n",
        "  return outputs\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHT380D9O4Z8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# BERT MODEL INITIALIZATION\n",
        "# source code can be found here https://github.com/huggingface/transformers/blob/bb7c46852051f7d031dd4be0240c9c9db82f6ed9/src/transformers/modeling_bert.py#L1107\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(os.path.join(PATH, \"imdb_weights\")) # load directly from checkpoint of imdb\n",
        "model.to(device) # move to gpu\n",
        "\n",
        "# PROCESS SIMPLE SENTENCE\n",
        "BERTMODEL = \"bert-base-uncased\"\n",
        "tokenizer = BertTokenizer.from_pretrained(BERTMODEL, do_lower_case=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9IMqO1NQYPe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EXTRACTED_ACTIVATIONS = []\n",
        "RECORD = False\n",
        "def extract_activation_hook(module, input, output):\n",
        "  if RECORD:\n",
        "    EXTRACTED_ACTIVATIONS.append(output)\n",
        "\n",
        "def add_activation_hook(model, layer_idx):\n",
        "  all_modules_list = list(model.modules())\n",
        "  module = all_modules_list[layer_idx]\n",
        "  module.register_forward_hook(extract_activation_hook)\n",
        "\n",
        "add_activation_hook(model, layer_idx=-2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqLdja1ZZuHj",
        "colab_type": "text"
      },
      "source": [
        "Notice here: the function above added a forward hook to \"layer_idx\" layer of our model. You might want to google \"register_forward_hook\" to fully understand it but in short, everytime something is fed into the model and through the layer we specified, the function \"extract_activation_hook\" will get called. And \"extract_activation_hook\" will save the layer output to EXTRACTED_ACTIVATIONS when RECORD is true."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gg8ejJYfLNL7",
        "colab_type": "code",
        "outputId": "b50c90d9-d209-4a8e-bcdb-753a68d3cacc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "### Old implementation, which is not the easiest way\n",
        "# MODEL DECOMPOSITION INTO PIECES\n",
        "# f_net = model\n",
        "# f_module_list = list(f_net.modules())\n",
        "# NUM_H_LAYERS = 1\n",
        "# # input -> activation net\n",
        "# theta_net = nn.Sequential(*f_module_list[:-NUM_H_LAYERS]) # everything except the last NUM_H_LAYERS\n",
        "# # activation -> result net\n",
        "# h_net = nn.Sequential(*f_module_list[-NUM_H_LAYERS:]) # the last NUM_H_LAYERS\n",
        "MAX_LEN_TRAIN, MAX_LEN_TEST = 128, 512\n",
        "\n",
        "\n",
        "global RECORD\n",
        "global EXTRACTED_ACTIVATIONS\n",
        "test_sentence = {}\n",
        "test_sentence[\"sentence\"] = [\"This movie is great\"]\n",
        "train_df = pd.DataFrame.from_dict(test_sentence)\n",
        "train_df[\"polarity\"] = [0]\n",
        "loader = process_dataframe(train_df, tokenizer)\n",
        "\n",
        "RECORD=True\n",
        "run_model(model, loader) # run the whole model\n",
        "RECORD=False\n",
        "\n",
        "print(len(EXTRACTED_ACTIVATIONS))\n",
        "print(\"the number above shall be increased by 1 each time we run this cell, since element will be appended to EXTRACTED_ACTIVATIONS everytime model is called\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n",
            "the number above shall be increased by 1 each time we run this cell, since element will be appended to EXTRACTED_ACTIVATIONS everytime model is called\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77XGd3aDY45K",
        "colab_type": "code",
        "outputId": "d582ce85-a833-424b-8f9c-6cddf2e1b689",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "global EXTRACTED_ACTIVATIONS\n",
        "EXTRACTED_ACTIVATIONS = []\n",
        "print(\"running this cell will clear the EXTRACTED_ACTIVATIONS\")\n",
        "print(len(EXTRACTED_ACTIVATIONS))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "running this cell will clear the EXTRACTED_ACTIVATIONS\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JO4cVLJZA5A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "047ztTZEZA7d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhPw3qNYZA9W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3Ps9J5CZB26",
        "colab_type": "text"
      },
      "source": [
        "# Look at our model and see if the activation we extracted make sense\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLsdfTUNLNOE",
        "colab_type": "code",
        "outputId": "0901ed80-c52d-4d5e-e7e3-9692150ed243",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(EXTRACTED_ACTIVATIONS[-1].shape) # pick one of the extracted activation\n",
        "print(\"this matches the input size of last linear layer, see the print out model in the cell below\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 768])\n",
            "this matches the input size of last linear layer, see the print out model in the cell below\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4WS4DHXZMOo",
        "colab_type": "code",
        "outputId": "07674b97-3379-4799-8792-8c07645a40a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dh4cGYpTZMTb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNMahXrCZMVz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWw5FMGIZMX6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgo_hVbSZWk9",
        "colab_type": "text"
      },
      "source": [
        "# Sanity checks on trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejNuSUsoLNQA",
        "colab_type": "code",
        "outputId": "db1026d2-3eb0-4548-e4e7-7b10c5a66cfe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# sanity checks\n",
        "\n",
        "\n",
        "MAX_LEN_TRAIN, MAX_LEN_TEST = 128, 512\n",
        "test_sentence = {}\n",
        "test_sentence[\"sentence\"] = [\"This movie is great\"]\n",
        "train_df = pd.DataFrame.from_dict(test_sentence)\n",
        "train_df[\"polarity\"] = [0]\n",
        "loader = process_dataframe(train_df, tokenizer)\n",
        "\n",
        "print(run_model(model, loader))\n",
        "\n",
        "MAX_LEN_TRAIN, MAX_LEN_TEST = 128, 512\n",
        "test_sentence = {}\n",
        "test_sentence[\"sentence\"] = [\"This movie is awesome\"]\n",
        "train_df = pd.DataFrame.from_dict(test_sentence)\n",
        "train_df[\"polarity\"] = [0]\n",
        "loader = process_dataframe(train_df, tokenizer)\n",
        "\n",
        "print(run_model(model, loader))\n",
        "\n",
        "MAX_LEN_TRAIN, MAX_LEN_TEST = 128, 512\n",
        "test_sentence = {}\n",
        "test_sentence[\"sentence\"] = [\"This movie is shit\"]\n",
        "train_df = pd.DataFrame.from_dict(test_sentence)\n",
        "train_df[\"polarity\"] = [0]\n",
        "loader = process_dataframe(train_df, tokenizer)\n",
        "\n",
        "print(run_model(model, loader))\n",
        "\n",
        "MAX_LEN_TRAIN, MAX_LEN_TEST = 128, 512\n",
        "test_sentence = {}\n",
        "test_sentence[\"sentence\"] = [\"This movie is messed up\"]\n",
        "train_df = pd.DataFrame.from_dict(test_sentence)\n",
        "train_df[\"polarity\"] = [0]\n",
        "loader = process_dataframe(train_df, tokenizer)\n",
        "\n",
        "print(run_model(model, loader))\n",
        "\n",
        "\n",
        "print(\"the result make sense. Trained model shall be loaded successfully\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(tensor([[-0.6978,  0.8706]], device='cuda:0'),)\n",
            "(tensor([[-0.7576,  0.1183]], device='cuda:0'),)\n",
            "(tensor([[ 0.9846, -1.3081]], device='cuda:0'),)\n",
            "(tensor([[ 1.0426, -1.1694]], device='cuda:0'),)\n",
            "the result make sense. Trained model shall be loaded successfully\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAxsEc2RLNR7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NE4oOZEEd8Kd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}